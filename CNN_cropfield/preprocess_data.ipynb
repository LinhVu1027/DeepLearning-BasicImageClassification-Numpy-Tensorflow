{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import h5py\n",
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing ./train/LuaCoDe.hdf5.\n",
      "('Full dataset tensor:', (7168, 256, 256, 3))\n",
      "Storing ./train/LuaNga.hdf5.\n",
      "('Full dataset tensor:', (20512, 256, 256, 3))\n",
      "Storing ./train/LuaTot.hdf5.\n",
      "('Full dataset tensor:', (17792, 256, 256, 3))\n"
     ]
    }
   ],
   "source": [
    "IMAGE_WIDTH = 256\n",
    "IMAGE_HEIGHT = 256\n",
    "CHANNEL = 3\n",
    "PIXEL_DEPTH = 255.0\n",
    "\n",
    "def load_image_from_folder(folder):\n",
    "    image_files = os.listdir(folder)\n",
    "    dataset = np.ndarray(shape=(len(image_files), IMAGE_WIDTH, IMAGE_HEIGHT, CHANNEL),\n",
    "                         dtype=np.float32)\n",
    "    \n",
    "    index_image = 0\n",
    "    for image in image_files:\n",
    "        image_file = os.path.join(folder, image)\n",
    "    \n",
    "        try:\n",
    "            image_data = (ndimage.imread(image_file).astype(float) - PIXEL_DEPTH/2) / PIXEL_DEPTH\n",
    "\n",
    "            if image_data.shape != (IMAGE_WIDTH, IMAGE_HEIGHT, CHANNEL):\n",
    "                raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "\n",
    "            dataset[index_image, :, :, :] = image_data\n",
    "            index_image = index_image + 1\n",
    "\n",
    "        except IOError as e:\n",
    "            print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "        \n",
    "    print('Full dataset tensor:', dataset.shape)\n",
    "    return dataset  \n",
    "\n",
    "def store_in_file(folders, force=False):\n",
    "    dataset_names = []\n",
    "    for folder in folders:\n",
    "        set_filename = folder + '.hdf5'\n",
    "        dataset_names.append(set_filename)\n",
    "        \n",
    "        if os.path.exists(set_filename) and not force:\n",
    "            print('%s already present - Skipping storing.' % set_filename)\n",
    "        else:\n",
    "            print('Storing %s.' % set_filename)\n",
    "            dataset = load_image_from_folder(folder)\n",
    "            try:\n",
    "                f = h5py.File(set_filename, \"w\")\n",
    "                f.create_dataset(\"ds\", data=dataset)\n",
    "                f.close()\n",
    "            except Exception as e:\n",
    "                print('Unable to save data to', set_filename, ':', e)\n",
    "    \n",
    "    return dataset_names\n",
    "\n",
    "train_datasets = store_in_file(['./train/LuaCoDe', './train/LuaNga', './train/LuaTot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training:', (18000, 256, 256, 3), (18000,))\n",
      "('Validation:', (3000, 256, 256, 3), (3000,))\n"
     ]
    }
   ],
   "source": [
    "def make_placeholder(n_examples, img_width, img_height, channel):\n",
    "    if n_examples:\n",
    "        dataset = np.ndarray((n_examples, img_width, img_height, channel), dtype=np.float32)\n",
    "        label = np.ndarray(n_examples, dtype=np.int32)\n",
    "    else:\n",
    "        dataset, label = None, None\n",
    "    return dataset, label\n",
    "\n",
    "def merge_datasets(hdf5_files, train_size, valid_size=0):\n",
    "    num_classes = len(hdf5_files)\n",
    "    train_dataset, train_labels = make_placeholder(train_size, IMAGE_WIDTH, IMAGE_HEIGHT, CHANNEL)\n",
    "    valid_dataset, valid_labels = make_placeholder(valid_size, IMAGE_WIDTH, IMAGE_HEIGHT, CHANNEL)\n",
    "  \n",
    "    tsize_per_class = train_size // num_classes\n",
    "    vsize_per_class = valid_size // num_classes   \n",
    "    \n",
    "    start_t, start_v = 0, 0\n",
    "    end_t, end_v = tsize_per_class, vsize_per_class\n",
    "    end_l = tsize_per_class + vsize_per_class \n",
    "    \n",
    "    for label, hdf5_file in enumerate(hdf5_files):       \n",
    "        try:\n",
    "            f = h5py.File(hdf5_file, 'r')\n",
    "            crop_set = f[\"ds\"][...]\n",
    "            np.random.shuffle(crop_set)\n",
    "            if valid_dataset is not None:\n",
    "                valid_crop = crop_set[:vsize_per_class, :, :, :]\n",
    "                valid_dataset[start_v:end_v, :, :, :] = valid_crop\n",
    "                valid_labels[start_v:end_v] = label\n",
    "                start_v += vsize_per_class\n",
    "                end_v += vsize_per_class\n",
    "                    \n",
    "            train_crop = crop_set[vsize_per_class:end_l, :, :, :]\n",
    "            train_dataset[start_t:end_t, :, :, :] = train_crop\n",
    "            train_labels[start_t:end_t] = label\n",
    "            start_t += tsize_per_class\n",
    "            end_t += tsize_per_class\n",
    "            f.close()\n",
    "        except Exception as e:\n",
    "            print('Unable to process data from', hdf5_file, ':', e)\n",
    "\n",
    "    \n",
    "    return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 18000\n",
    "valid_size = 3000\n",
    "#test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "#_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "#print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:,:]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)\n",
    "#test_dataset, test_labels = randomize(test_dataset, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf5_file = \"cropfield.hdf5\"\n",
    "\n",
    "try:\n",
    "    f = h5py.File(hdf5_file, 'w')\n",
    "#     save = {\n",
    "#         'train_dataset': train_dataset,\n",
    "#         'train_labels': train_labels,\n",
    "#         'valid_dataset': valid_dataset,\n",
    "#         'valid_labels': valid_labels,\n",
    "#         'test_dataset': test_dataset,\n",
    "#         'test_labels': test_labels,\n",
    "#         }\n",
    "    f.create_dataset(\"train_dataset\", data=train_dataset)\n",
    "    f.create_dataset(\"train_labels\", data=train_labels)\n",
    "    f.create_dataset(\"valid_dataset\", data=valid_dataset)\n",
    "    f.create_dataset(\"valid_labels\", data=valid_labels)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', hdf5_file, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Compressed hdf5 size:', 15)\n"
     ]
    }
   ],
   "source": [
    "statinfo = os.stat(hdf5_file)\n",
    "print('Compressed hdf5 size:', statinfo.st_size/1048576/1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
